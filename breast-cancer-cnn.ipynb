{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-26T06:55:19.211106Z","iopub.execute_input":"2023-09-26T06:55:19.211519Z","iopub.status.idle":"2023-09-26T06:56:36.926932Z","shell.execute_reply.started":"2023-09-26T06:55:19.211488Z","shell.execute_reply":"2023-09-26T06:56:36.925879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Data","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:57:55.257917Z","iopub.execute_input":"2023-09-26T06:57:55.258303Z","iopub.status.idle":"2023-09-26T06:57:56.066979Z","shell.execute_reply.started":"2023-09-26T06:57:55.258269Z","shell.execute_reply":"2023-09-26T06:57:56.066002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_meta = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/meta.csv')\ndf_meta.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:57:59.837571Z","iopub.execute_input":"2023-09-26T06:57:59.837931Z","iopub.status.idle":"2023-09-26T06:57:59.928272Z","shell.execute_reply.started":"2023-09-26T06:57:59.837900Z","shell.execute_reply":"2023-09-26T06:57:59.927118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load dicom info file\ndf_dicom = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/dicom_info.csv')\ndf_dicom.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:02.737319Z","iopub.execute_input":"2023-09-26T06:58:02.737687Z","iopub.status.idle":"2023-09-26T06:58:02.957754Z","shell.execute_reply.started":"2023-09-26T06:58:02.737657Z","shell.execute_reply":"2023-09-26T06:58:02.956773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check image types in dataset\ndf_dicom.SeriesDescription.unique()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:06.162814Z","iopub.execute_input":"2023-09-26T06:58:06.163204Z","iopub.status.idle":"2023-09-26T06:58:06.177301Z","shell.execute_reply.started":"2023-09-26T06:58:06.163169Z","shell.execute_reply":"2023-09-26T06:58:06.176173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check image path in dataset\n# cropped images\ncropped_images = df_dicom[df_dicom.SeriesDescription=='cropped images'].image_path\ncropped_images.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:09.371500Z","iopub.execute_input":"2023-09-26T06:58:09.372463Z","iopub.status.idle":"2023-09-26T06:58:09.386503Z","shell.execute_reply.started":"2023-09-26T06:58:09.372420Z","shell.execute_reply":"2023-09-26T06:58:09.385204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#full mammogram images\nfull_mammo = df_dicom[df_dicom.SeriesDescription=='full mammogram images'].image_path\nfull_mammo.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:11.922577Z","iopub.execute_input":"2023-09-26T06:58:11.922941Z","iopub.status.idle":"2023-09-26T06:58:11.936499Z","shell.execute_reply.started":"2023-09-26T06:58:11.922910Z","shell.execute_reply":"2023-09-26T06:58:11.935193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROI images\nroi_img = df_dicom[df_dicom.SeriesDescription=='ROI mask images'].image_path\nroi_img.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:13.964422Z","iopub.execute_input":"2023-09-26T06:58:13.964808Z","iopub.status.idle":"2023-09-26T06:58:13.978806Z","shell.execute_reply.started":"2023-09-26T06:58:13.964777Z","shell.execute_reply":"2023-09-26T06:58:13.977566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set correct image path for image types\nimdir = '../input/cbis-ddsm-breast-cancer-image-dataset/jpeg'","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:18.438514Z","iopub.execute_input":"2023-09-26T06:58:18.439266Z","iopub.status.idle":"2023-09-26T06:58:18.444342Z","shell.execute_reply.started":"2023-09-26T06:58:18.439219Z","shell.execute_reply":"2023-09-26T06:58:18.443284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change directory path of images\ncropped_images = cropped_images.replace('CBIS-DDSM/jpeg', imdir, regex=True)\nfull_mammo = full_mammo.replace('CBIS-DDSM/jpeg', imdir, regex=True)\nroi_img = roi_img.replace('CBIS-DDSM/jpeg', imdir, regex=True)\n\n# view new paths\nprint('Cropped Images paths:\\n')\nprint(cropped_images.iloc[0])\nprint('Full mammo Images paths:\\n')\nprint(full_mammo.iloc[0])\nprint('ROI Mask Images paths:\\n')\nprint(roi_img.iloc[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:22.253046Z","iopub.execute_input":"2023-09-26T06:58:22.253749Z","iopub.status.idle":"2023-09-26T06:58:22.285638Z","shell.execute_reply.started":"2023-09-26T06:58:22.253712Z","shell.execute_reply":"2023-09-26T06:58:22.284605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# organize image paths\nfull_mammo_dict = dict()\ncropped_images_dict = dict()\nroi_img_dict = dict()\n\nfor dicom in full_mammo:\n    key = dicom.split(\"/\")[4]\n    full_mammo_dict[key] = dicom\nfor dicom in cropped_images:\n    key = dicom.split(\"/\")[4]\n    cropped_images_dict[key] = dicom\nfor dicom in roi_img:\n    key = dicom.split(\"/\")[4]\n    roi_img[key] = dicom\n\n# view keys\nnext(iter((full_mammo_dict.items())))","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:58:36.946333Z","iopub.execute_input":"2023-09-26T06:58:36.946778Z","iopub.status.idle":"2023-09-26T06:58:50.598552Z","shell.execute_reply.started":"2023-09-26T06:58:36.946738Z","shell.execute_reply":"2023-09-26T06:58:50.597570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mass Dataset","metadata":{}},{"cell_type":"code","source":"# load the mass dataset\nmass_train = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_train_set.csv')\nmass_test = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_test_set.csv')\n\nmass_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:07.677285Z","iopub.execute_input":"2023-09-26T06:59:07.677673Z","iopub.status.idle":"2023-09-26T06:59:07.744004Z","shell.execute_reply.started":"2023-09-26T06:59:07.677638Z","shell.execute_reply":"2023-09-26T06:59:07.742922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fix image paths\ndef fix_image_path(data):\n    \"\"\"correct dicom paths to correct image paths\"\"\"\n    for index, img in enumerate(data.values):\n        img_name = img[11].split(\"/\")[2]\n        data.iloc[index,11] = full_mammo_dict[img_name]\n        img_name = img[12].split(\"/\")[2]\n        data.iloc[index,12] = cropped_images_dict[img_name]\n        \n# apply to datasets\nfix_image_path(mass_train)\nfix_image_path(mass_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:13.688751Z","iopub.execute_input":"2023-09-26T06:59:13.689108Z","iopub.status.idle":"2023-09-26T06:59:14.032463Z","shell.execute_reply.started":"2023-09-26T06:59:13.689076Z","shell.execute_reply":"2023-09-26T06:59:14.031369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check unique values in pathology column\nmass_train.pathology.unique()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:17.740028Z","iopub.execute_input":"2023-09-26T06:59:17.740703Z","iopub.status.idle":"2023-09-26T06:59:17.748848Z","shell.execute_reply.started":"2023-09-26T06:59:17.740669Z","shell.execute_reply":"2023-09-26T06:59:17.747659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mass_train.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:21.846180Z","iopub.execute_input":"2023-09-26T06:59:21.847664Z","iopub.status.idle":"2023-09-26T06:59:21.871749Z","shell.execute_reply.started":"2023-09-26T06:59:21.847623Z","shell.execute_reply":"2023-09-26T06:59:21.870456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rename columns\nmass_train = mass_train.rename(columns={'left or right breast': 'left_or_right_breast',\n                                           'image view': 'image_view',\n                                           'abnormality id': 'abnormality_id',\n                                           'abnormality type': 'abnormality_type',\n                                           'mass shape': 'mass_shape',\n                                           'mass margins': 'mass_margins',\n                                           'image file path': 'image_file_path',\n                                           'cropped image file path': 'cropped_image_file_path',\n                                           'ROI mask file path': 'ROI_mask_file_path'})\n\nmass_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:26.661620Z","iopub.execute_input":"2023-09-26T06:59:26.661986Z","iopub.status.idle":"2023-09-26T06:59:26.681511Z","shell.execute_reply.started":"2023-09-26T06:59:26.661954Z","shell.execute_reply":"2023-09-26T06:59:26.680384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for null values\nmass_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:30.221604Z","iopub.execute_input":"2023-09-26T06:59:30.221958Z","iopub.status.idle":"2023-09-26T06:59:30.234150Z","shell.execute_reply.started":"2023-09-26T06:59:30.221927Z","shell.execute_reply":"2023-09-26T06:59:30.233168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill in missing values using the backwards fill method\nmass_train['mass_shape'] = mass_train['mass_shape'].fillna(method='bfill')\nmass_train['mass_margins'] = mass_train['mass_margins'].fillna(method='bfill')\n\n#check null values\nmass_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:33.388947Z","iopub.execute_input":"2023-09-26T06:59:33.389387Z","iopub.status.idle":"2023-09-26T06:59:33.413912Z","shell.execute_reply.started":"2023-09-26T06:59:33.389354Z","shell.execute_reply":"2023-09-26T06:59:33.412666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quantitative summary of features\nmass_train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:37.359736Z","iopub.execute_input":"2023-09-26T06:59:37.360221Z","iopub.status.idle":"2023-09-26T06:59:37.392871Z","shell.execute_reply.started":"2023-09-26T06:59:37.360183Z","shell.execute_reply":"2023-09-26T06:59:37.391822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view mass_test\nmass_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:43.728932Z","iopub.execute_input":"2023-09-26T06:59:43.729304Z","iopub.status.idle":"2023-09-26T06:59:43.746810Z","shell.execute_reply.started":"2023-09-26T06:59:43.729273Z","shell.execute_reply":"2023-09-26T06:59:43.745859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check datasets shape\nprint(f'Shape of mass_train: {mass_train.shape}')\nprint(f'Shape of mass_test: {mass_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-09-26T06:59:55.150292Z","iopub.execute_input":"2023-09-26T06:59:55.151343Z","iopub.status.idle":"2023-09-26T06:59:55.157649Z","shell.execute_reply.started":"2023-09-26T06:59:55.151296Z","shell.execute_reply":"2023-09-26T06:59:55.156637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mass_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:00:02.800061Z","iopub.execute_input":"2023-09-26T07:00:02.800537Z","iopub.status.idle":"2023-09-26T07:00:02.810046Z","shell.execute_reply.started":"2023-09-26T07:00:02.800496Z","shell.execute_reply":"2023-09-26T07:00:02.809038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for column names in mass_test\nprint(mass_test.columns)\nprint('\\n')\n# rename columns\nmass_test = mass_test.rename(columns={'left or right breast': 'left_or_right_breast',\n                                           'image view': 'image_view',\n                                           'abnormality id': 'abnormality_id',\n                                           'abnormality type': 'abnormality_type',\n                                           'mass shape': 'mass_shape',\n                                           'mass margins': 'mass_margins',\n                                           'image file path': 'image_file_path',\n                                           'cropped image file path': 'cropped_image_file_path',\n                                           'ROI mask file path': 'ROI_mask_file_path'})\n\n# view renamed columns\nmass_test.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:00:07.289831Z","iopub.execute_input":"2023-09-26T07:00:07.290227Z","iopub.status.idle":"2023-09-26T07:00:07.300728Z","shell.execute_reply.started":"2023-09-26T07:00:07.290193Z","shell.execute_reply":"2023-09-26T07:00:07.299805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill in missing values using the backwards fill method\nmass_test['mass_margins'] = mass_test['mass_margins'].fillna(method='bfill')\n\n#check null values\nmass_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:00:10.890845Z","iopub.execute_input":"2023-09-26T07:00:10.891945Z","iopub.status.idle":"2023-09-26T07:00:10.904275Z","shell.execute_reply.started":"2023-09-26T07:00:10.891900Z","shell.execute_reply":"2023-09-26T07:00:10.902942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"code","source":"# pathology distributions\nvalue = mass_train['pathology'].value_counts()\nplt.figure(figsize=(8,8))\n\nplt.pie(value, labels=value.index, autopct='%1.1f%%')\nplt.title('Breast Cancer Mass Types', fontsize=14)\n#plt.savefig('/kaggle/working/pathology_distributions.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:05:04.160323Z","iopub.execute_input":"2023-09-26T07:05:04.160885Z","iopub.status.idle":"2023-09-26T07:05:04.476015Z","shell.execute_reply.started":"2023-09-26T07:05:04.160842Z","shell.execute_reply":"2023-09-26T07:05:04.474910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# examine breast assessment types\nplt.figure(figsize=(10,8))\nsns.countplot(mass_train, y='assessment', hue='pathology', palette='viridis')\nplt.title('Breast Cancer Assessment\\n\\n 0: Undetermined || 1: Well Differentiated\\n2: Moderately differentiated || 3: Poorly DIfferentiated\\n4-5: Undifferentiated', \n          fontsize=12)\nplt.ylabel('Assessment Grade')\nplt.xlabel('Count')\n#plt.savefig('/kaggle/working/breast_assessment.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:05:27.397904Z","iopub.execute_input":"2023-09-26T07:05:27.398290Z","iopub.status.idle":"2023-09-26T07:05:27.938721Z","shell.execute_reply.started":"2023-09-26T07:05:27.398258Z","shell.execute_reply":"2023-09-26T07:05:27.937808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# examine cancer subtlety\nplt.figure(figsize=(10,8))\nsns.countplot(mass_train, x='subtlety', palette='viridis')\nplt.title('Breast Cancer Mass Subtlety', fontsize=12)\nplt.xlabel('Subtlety Grade')\nplt.ylabel('Count')\n#plt.savefig('/kaggle/working/cancer_subtlety.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:05:51.624272Z","iopub.execute_input":"2023-09-26T07:05:51.624642Z","iopub.status.idle":"2023-09-26T07:05:52.018068Z","shell.execute_reply.started":"2023-09-26T07:05:51.624609Z","shell.execute_reply":"2023-09-26T07:05:52.017121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view breast mass shape distribution against pathology\nplt.figure(figsize=(10,8))\n\nsns.countplot(mass_train, x='mass_shape', hue='pathology')\nplt.title('Mass Shape Distribution by Pathology', fontsize=14)\nplt.xlabel('Mass Shape')\nplt.xticks(rotation=30, ha='right')\nplt.ylabel('Pathology Count')\nplt.legend()\n#plt.savefig('/kaggle/working/mass_pathology.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:06:09.555172Z","iopub.execute_input":"2023-09-26T07:06:09.555538Z","iopub.status.idle":"2023-09-26T07:06:10.395821Z","shell.execute_reply.started":"2023-09-26T07:06:09.555510Z","shell.execute_reply":"2023-09-26T07:06:10.394897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# breast density against pathology\nplt.figure(figsize=(10,8))\n\nsns.countplot(mass_train, x='breast_density', hue='pathology')\nplt.title('Breast Density vs Pathology\\n\\n1: fatty || 2: Scattered Fibroglandular Density\\n3: Heterogenously Dense || 4: Extremely Dense',\n          fontsize=14)\nplt.xlabel('Density Grades')\nplt.ylabel('Count')\nplt.legend()\n#plt.savefig('/kaggle/working/density_pathology.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:06:35.032001Z","iopub.execute_input":"2023-09-26T07:06:35.032691Z","iopub.status.idle":"2023-09-26T07:06:35.528070Z","shell.execute_reply.started":"2023-09-26T07:06:35.032655Z","shell.execute_reply":"2023-09-26T07:06:35.527026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display some images\nimport matplotlib.image as mpimg\n\n# create function to display images\ndef display_images(column, number):\n    \"\"\"displays images in dataset\"\"\"\n    # create figure and axes\n    number_to_visualize = number\n    rows = 1\n    cols = number_to_visualize\n    fig, axes = plt.subplots(rows, cols, figsize=(15,5))\n    \n    # Loop through rows and display images\n    for index, row in mass_train.head(number_to_visualize).iterrows():\n        image_path = row[column]\n        image = mpimg.imread(image_path)\n        ax = axes[index]\n        ax.imshow(image, cmap='gray')\n        ax.set_title(f\"{row['pathology']}\")\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nprint('Full Mammograms:\\n')\ndisplay_images('image_file_path', 5)\nprint('Cropped Mammograms:\\n')\ndisplay_images('cropped_image_file_path', 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:10:11.038938Z","iopub.execute_input":"2023-09-26T07:10:11.039317Z","iopub.status.idle":"2023-09-26T07:10:16.598635Z","shell.execute_reply.started":"2023-09-26T07:10:11.039284Z","shell.execute_reply":"2023-09-26T07:10:16.597147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing of Images","metadata":{}},{"cell_type":"code","source":"import tensorflow\nimport cv2\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\ndef image_processor(image_path, target_size):\n    \"\"\"Preprocess images for CNN model\"\"\"\n    #print(\"Image path:\", image_path)  # Print the value of image_path\n    absolute_image_path = os.path.abspath(image_path)\n    image = cv2.imread(absolute_image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n    image = cv2.resize(image, (target_size[1], target_size[0]))  # Resize the image\n    image_array = image / 255.0  # Normalize pixels\n    return image_array","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:12:05.370975Z","iopub.execute_input":"2023-09-26T07:12:05.372046Z","iopub.status.idle":"2023-09-26T07:12:05.380629Z","shell.execute_reply.started":"2023-09-26T07:12:05.372007Z","shell.execute_reply":"2023-09-26T07:12:05.379299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train_Test_Validation","metadata":{}},{"cell_type":"code","source":"# merge datasets\nfull_mass = pd.concat([mass_train, mass_test], axis=0)\n\n# Define the target size\ntarget_size = (224, 224, 3)\n\n# apply preprocessor to train data\nfull_mass['processed_images'] = full_mass['image_file_path'].apply(lambda x: image_processor(x, target_size))\n\n# create a binary mapper\nclass_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0} \n\n# Convert the processed_images column to an array\nX_resized = np.array(full_mass['processed_images'].tolist())\n\n# Verify the shape of the resized array\nprint(X_resized.shape)\n\n# apply class mapper to pathology column\nfull_mass['labels'] = full_mass['pathology'].replace(class_mapper)\n\n# check number of classes\nnum_classes = len(full_mass['labels'].unique())\n\n# set customary feature and target variables\nX = X_resized\ny = full_mass['labels'].values\n\n# Reshape X to include the number of samples\n#num_samples = X.shape[0]\nX = X.reshape(-1, 224, 224, 3)\n\n# Split data into train, test, and validation sets (70, 20, 10)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:12:15.784700Z","iopub.execute_input":"2023-09-26T07:12:15.785055Z","iopub.status.idle":"2023-09-26T07:17:34.522843Z","shell.execute_reply.started":"2023-09-26T07:12:15.785024Z","shell.execute_reply":"2023-09-26T07:17:34.521707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert integer labels to one-hot encoded labels\ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)\ny_val = to_categorical(y_val, num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:17:48.260790Z","iopub.execute_input":"2023-09-26T07:17:48.261180Z","iopub.status.idle":"2023-09-26T07:17:48.266925Z","shell.execute_reply.started":"2023-09-26T07:17:48.261126Z","shell.execute_reply":"2023-09-26T07:17:48.265679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_labels = full_mass['labels'].unique()\nnum_unique_labels = len(unique_labels)\n\nprint(\"Unique labels:\", unique_labels)\nprint(\"Number of unique labels:\", num_unique_labels)\nprint(\"Num classes:\", num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:17:53.452227Z","iopub.execute_input":"2023-09-26T07:17:53.452619Z","iopub.status.idle":"2023-09-26T07:17:53.459214Z","shell.execute_reply.started":"2023-09-26T07:17:53.452588Z","shell.execute_reply":"2023-09-26T07:17:53.457934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Architecture","metadata":{}},{"cell_type":"code","source":"# import necessary tensorflow libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\n\n# Augment data\ntrain_datagen = ImageDataGenerator(rotation_range=40, \n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest'\n                                 )\n\n# apply augmentation to training data\ntrain_data_augmented = train_datagen.flow(X_train, y_train, batch_size=16)\n\n# instantiate CNN model\nmodel = Sequential()\n\n# add layers\nmodel.add(Conv2D(32, (3, 3), activation='relu',\n                input_shape=(224, 224, 3)))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(64, (3,3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten()) # flatten feature maps\nmodel.add(Dense(512, activation='relu')) # add fully connected layers\nmodel.add(Dense(num_classes, activation='softmax')) # output layer\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam', \n              metrics=['accuracy'])\n\n# train model\nhistory = model.fit(train_data_augmented,\n                    epochs=30, \n                    validation_data=(X_val, y_val)\n                   )\n\n# save model architecture as png file\nplot_model(model, to_file='model-1_architecture.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T07:20:17.640176Z","iopub.execute_input":"2023-09-26T07:20:17.640567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model summary\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# increase epoch, batch size, reduce number of layers, add dropout ratio\n\ntrain_data_augmented_2 = train_datagen.flow(X_train, y_train, batch_size=32)\n\n# instantiate second model\nmodel_2 = Sequential()\n\n# Add layers\nmodel_2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\nmodel_2.add(MaxPooling2D((2, 2)))\nmodel_2.add(Dropout(0.25))\n\nmodel_2.add(Conv2D(64, (3,3), activation='relu'))\nmodel_2.add(MaxPooling2D((2, 2)))\nmodel_2.add(Dropout(0.25))\n\nmodel_2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2.add(MaxPooling2D((2, 2)))\nmodel_2.add(Dropout(0.25))\n\nmodel_2.add(Conv2D(128, (3, 3), activation='relu'))\nmodel_2.add(MaxPooling2D((2, 2)))\nmodel_2.add(Dropout(0.25))\n\nmodel_2.add(Flatten())\nmodel_2.add(Dense(128, activation='relu'))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(Dense(num_classes, activation='softmax'))\n\n# compile\nmodel_2.compile(optimizer=Adam(lr=0.0001), \n                loss='binary_crossentropy', \n                metrics=['accuracy'])\n\n# fit model\nhistory_2 = model_2.fit(train_data_augmented_2,\n                    epochs=35,\n                    validation_data=(X_val, y_val)\n                   )\n\n# save model architecture\nplot_model(model_2, to_file='model-2_architecture.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model summary\nmodel_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"model_2.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\n# create labels for confusion matrix\ncm_labels = ['MALIGNANT', 'BENIGN']\n\n# obtain predictions\ny_pred_test = model.predict(X_test)\ny_pred_val = model.predict(X_val)\n\n# convert predicted probabilities to class predictions\ny_pred_classes_test = np.argmax(y_pred_test, axis=1)\ny_pred_classes_val = np.argmax(y_pred_val, axis=1)\n\n# Assuming y_test and y_val are in binary format (0 or 1)\ny_true_classes_test = np.argmax(y_test, axis=1)\ny_true_classes_val = np.argmax(y_val, axis=1)\n\n# generate classification reports for test and val sets\ntest_report = classification_report(y_true_classes_test, y_pred_classes_test, target_names=cm_labels)\nval_report = classification_report(y_true_classes_val, y_pred_classes_val, target_names=cm_labels)\n\n# generate confusion matrices for test and validation sets\ntest_cm = confusion_matrix(y_true_classes_test, y_pred_classes_test)\nval_cm = confusion_matrix(y_true_classes_val, y_pred_classes_val)\n\n# create function to print confusion matrix\ndef plot_confusion_matrix(cm, labels, title):\n    \"\"\"plots confusion matrix\"\"\"\n    plt.figure(figsize=(8, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=labels, yticklabels=labels)\n    plt.title(title, fontsize=14)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n\n# print test and validation reports and matrices\nprint(f\"Test Set Classification report:\\n {test_report}\\n\")\nplot_confusion_matrix(test_cm, cm_labels, 'Test Set Confusion Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Validation Set Classification report:\\n {val_report}\\n\")\nplot_confusion_matrix(val_cm, cm_labels, 'Validation Set Confusion Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC_AUC Curves","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n# Use the trained model to predict probabilities for the test set\ny_pred_prob = model.predict(X_test)\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test[:, 1], y_pred_prob[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()\n\n# Print the AUC score\nprint(f'AUC: {roc_auc:.2f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Loss vs Epoch/Accuracy vs Epoch ","metadata":{}},{"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training loss vs validation loss\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nacc = history_dict['accuracy']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss_values, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_values, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss', fontsize=12)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n#history_df = pd.DataFrame(history.history)\n#history_df[['loss', 'val_loss']].plot()\n\n#history_df = pd.DataFrame(history.history)\n#history_df[['accuracy', 'val_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training vs validation accuracy\nval_acc_values = history_dict['val_accuracy']\nacc = history_dict['accuracy']\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc_values, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy', fontsize=12)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning","metadata":{}},{"cell_type":"code","source":"# use VGG19\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\n\ntrain_data_aug_3 = train_datagen.flow(X_train, y_train, batch_size=16)\n\nvgg_model = Sequential()\n\npretrained_model = VGG19(include_top=False, \n                         input_shape=(224, 224, 3), \n                         classes=num_classes, \n                         weights='imagenet')\n\n# apply GAP to last layer of pretrained model\npretrained_model.layers[-1] = GlobalAveragePooling2D()\n\nfor layer in pretrained_model.layers:\n    layer.trainable=False\n\n# add layers\nvgg_model.add(pretrained_model)\nvgg_model.add(Flatten())\nvgg_model.add(Dense(512, activation='relu'))\nvgg_model.add(Dense(num_classes, activation='softmax'))\n\n# train model\nvgg_model.compile(optimizer=Adam(lr=0.001), \n                    loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\n# fit model\nhistory_3 = vgg_model.fit(train_data_aug_3, \n                            epochs=30,\n                            validation_data=(X_val, y_val))\n\n# save architecture\nplot_model(vgg_model, to_file='transfer_learning-1_archictect.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model summary\nvgg_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Report: Transfer Learning","metadata":{}},{"cell_type":"code","source":"# classification report and confusion matrix\n\n#obtain predictions\ny_pred_test_res = vgg_model.predict(X_test)\ny_pred_val_res = vgg_model.predict(X_val)\n\n# convert predicted probabilities to class predictions\ny_pred_classes_test_res = np.argmax(y_pred_test_res, axis=1)\ny_pred_classes_val_res = np.argmax(y_pred_val_res, axis=1)\n\n# get true classes\ny_true_classes_test_res = np.argmax(y_test, axis=1)\ny_true_classes_val_res = np.argmax(y_val, axis=1)\n\n# generate classification report\ntest_report_res = classification_report(y_true_classes_test_res, y_pred_classes_test_res, target_names=cm_labels)\nval_report_res = classification_report(y_true_classes_val_res, y_pred_classes_val_res, target_names=cm_labels)\n\n# generate confusion matrix\ntest_cm_res = confusion_matrix(y_true_classes_test_res, y_pred_classes_test_res)\nval_cm_res = confusion_matrix(y_true_classes_val_res, y_pred_classes_val_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Test Set Classification report:\\n {test_report_res}\\n\")\nplot_confusion_matrix(test_cm_res, cm_labels, 'Test Set Confusion Matrix: VGG19')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Validation Set Classifcation report:\\n {val_report_res}\\n')\nplot_confusion_matrix(val_cm_res, cm_labels, 'Validation Set Confusion Matrix: VGG19')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC-AUC Curves: Transfer Learning","metadata":{}},{"cell_type":"code","source":"# ROC-AUC Curves\n\n# Use the trained model to predict probabilities for the test set\ny_pred_prob = vgg_model.predict(X_test)\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(y_test[:, 1], y_pred_prob[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# Plot the ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()\n\n# Print the AUC score\nprint(f'AUC: {roc_auc:.2f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Epochs-Loss-Accuracy Visualization: Transfer Learning","metadata":{}},{"cell_type":"code","source":"history_3_dict = history_3.history\nhistory_3_dict.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training loss vs validation loss\nloss_values = history_3_dict['loss']\nval_loss_values = history_3_dict['val_loss']\nacc = history_3_dict['accuracy']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss_values, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_values, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss', fontsize=12)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training vs validation accuracy\nval_acc_values = history_3_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc_values, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy', fontsize=12)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Demo","metadata":{}},{"cell_type":"code","source":"#image_pred = vgg_model.predict(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image_output_class = labels[np.argmax(image_pred)] # to obtain a human readable output\n#print('The predicted class is', image_output_class)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}